%        File: report.tex
%     Created: Tue Feb 07 05:00 PM 2017 C
% Last Change: Tue Feb 07 05:00 PM 2017 C
%

\documentclass[a4paper]{article}

\title{Computer Science 5451 Pagerank Report }
\date{3/6/17}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{changepage}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\div}{\mathrm{div}}

\newenvironment{solution}[1][]{\emph{Solution #1}}

\algnewcommand{\Or}{\textbf{ or }}
\algnewcommand{\And}{\textbf{ or }}

\begin{document}
\maketitle

\section{Parallelization}

In the Pagerank algorithm, there are two basic steps. First, each vertex must look through its outgoing edges and push its current Pagerank out along
each of these edges. Then, each vertex must combine the values pushed to it in order to obtain its new Pagerank value. In the parallel implementation,
the first step was accomplished by building an accumulator and storing the values to push in this accumulator. After filling the accumulator, the
necessary communication happens between processes, followed by the updating of Pagerank values using these communicated values.

In my implementation of parallel Pagerank, each process constructed an accumulator. The accumulator's purpose was to store the values being pushed
from each vertex in a given process in such a way to be easily communicated to other processes. One observation that speeds up the use of the accumulator is that the communication pattern is determined by the
graph and does not change from iteration to iteration. Therefore, the structures needed by the accumulator could be precomputed, reducing computation
time at each iteration.

To build the communication structures and accumulator, each process scanned its list of edges and added them to a large array. Then, this array was
sorted, and all repeated entries were removed. This shortened array gave the unique vertices that a process must push values to. Along with this
array, a set of arrays is constructed using the compressed sparse row format which gives the indices each local vertex must add its values to the
shortened accumulator array just constructed. This step uses a binary search of the global indices in the shortened array that only contains vertices
the current process will send values to. Using these arrays,
each processor determines how many values it would send to every process. These values are shared among all processes using an all-to-all broadcast.
From these values each process determines how many values it will receive from every process and where to place them in the array it uses for
receiving values.

After precomputing is done, each process loops over its vertices, computes the appropriate value to push to each vertex, and uses the accumulator and
associated indexing arrays to place the values being pushed in the correct locations. After this is done, values are communicated to the correct
locations through an all-to-all personalized operation. Upon receving its values, each process scans the values received along with a precomputed
indexing array to add the received values to the Pagerank of the correct local vertices.

One observation with presorting the index array in the implementation above is that it adds a computation that is asymptotically large in some cases. For each
iteration, the serial time required to compute new Pagerank values is $O(|E|)$. This aligns with the $O(\frac{|E|}{p})$ computation time to update
Pagerank values in the parallel algorithm (assuming a uniform splitting of edges). The use of presorting introduces a $O(\frac{|E|}{p} \log \frac{|E|}{p} )$ computation. When the number of
iterations becomes large, this precompution cost can be amortized across the iterations, but for small numbers of iterations, this
precomputing step dominates the computation time.

We will consider the communication cost associated to each iteration of the parallel Pagerank algorithm. Determining these costs is not a straight-forward task. This is because the communication
costs depend largely on the sparsity pattern of the graph. The communication required in the above computations is an all-to-all personalized
communication. The cost of which is
\[ (t_s + t_w m) (p - 1) \]
where $m$ is the size of the message sent from each node to every other node. There is also an all-to-all reduce in order to combine the local pieces
of the Frobenius norm of the difference in Pagerank vectors between two iterations, which is $t_s \log p + t_w m (p-1)$, but this is strictly smaller
than the term for the all-to-all personalized and will therefore be ignored. If the graph is well-behaved, each process may have roughly
$\frac{|E|}{p}$ outgoing edges, which are split evenly amongst the $p$ processes, giving $m = O \left( \frac{|E|}{p^2} \right)$. If this is the case,
when we combine with the $O \left( \frac{|E|}{p} \right)$ cost of computing the updated Pagerank values, we have a parallel runtime of
\[ T_p = O \left( \frac{|E|}{p} \right) t_c + O(p) t_s + O \left( \frac{|E|}{p} \right) t_w .\]
Multiplying by $p$, we get the processor-time product is of the same order as the serial runtime if $p^2 = O(|E|)$, leading to a cost-optimal
algorithm in this case. This analysis all depends on the assumptions made about the number of edges between different processes, which may not
necessarily hold.


\section{Results}

The following are the per iteration timing results of the Pagerank implementation with sorting in the precomputing to reduce communication time. The
serial implementation used is the one provided by Shaden. The parallel implementation run on a single process has a very similar runtime as the serial
implementation. There is a slightly longer running time, most likely due to an extra array lookup associated to indexing into the accumulator which
is set up to avoid storing zero values.

\begin{figure}[h]
  \begin{tabular}{| r | r | r r | r |}
    \hline
    \multicolumn{5}{|c|}{Pagerank per Iteration Timing Results} \\
    \hline
    Dataset & Processes & {Time/Iteration (sec)} & {Speedup} & Iterations\\
    \hline
    \multirow{6}{4em}{A.graph} & Serial & 0.144 & -- & \multirow{7}{4em}{80} \\
    & 1 & 0.177 & 0.814 & \\
    & 2 & 0.096 & 1.50 & \\
    & 4 & 0.046 & 3.13 & \\
    & 8 & 0.028 & 5.14 & \\
    & 16 & 0.016 & 9.00 & \\
    & 32 & 0.015 & 9.60 & \\
    \hline
    \multirow{6}{4em}{B.graph} & Serial & 0.931 & -- & \multirow{7}{4em}{12}\\
    & 1 & 0.950 & 0.980 & \\
    & 2 & 0.487 & 1.91 & \\
    & 4 & 0.330 & 2.82 & \\
    & 8 & 0.244 & 3.82 & \\
    & 16 & 0.235 & 3.96 & \\
    & 32 & 1.475 & 0.63 & \\
    \hline
    \multirow{6}{8em}{live-journal.graph} & Serial & 0.399 & -- & \multirow{7}{4em}{100} \\
    & 1 & 0.415 & 0.961 & \\
    & 2 & 0.253 & 1.57 & \\
    & 4 & 0.177 & 2.25 & \\
    & 8 & 0.122 & 2.92 & \\
    & 16 & 0.096 & 4.16 & \\
    & 32 & 0.303 & 1.32 & \\
    \hline
  \end{tabular}
\end{figure}

In the A.graph dataset, edges were such that a vertex pushed its values to vertices that were close in label. This meant that most of the values
pushed out from vertices were pushed to other vertices in the same process. This meant communication between processes was very small, allowing better
speedup.

B.graph and live-journal.graph each had a large number of edges between processes. This meant the communication costs were much higher for each
iteration. This communication cost became very apparent when using 32 processes. At this point, not all processes could be run on a single machine, so
the communication had to occur between different machines. This caused a significant drop in performance from that attained with 16 threads on these
two datasets.

One observation that was made during this assignment was that the precomputing often required a significant amount of time when compared to the actual
iterative process of calculating Pagerank. This stems from the fact that each iteration cost $O(|E|)$ for updates in the serial algorithm. The
preprocessing requires sorting the edge set, which is a $O({{|E|}\over{p}} \log {{|E|}\over{p}})$ operation. When the number of iterations for convergence
is small, this precomputing step ends up being the dominant term in the calculation.

To illustrate the above observation, timing was also done on the total runtime including precomputing. The serial implementation used is the one provided by Shaden. Values in columns
labeled ``With Sorting'' correspond to precomputing steps that presorted all outgoing edges to reduce message size. This was compared to an
implementation labeled ``Without Sorting'' which computed similar arrays for indexing local vertex push values into an accumulator, but the
precomputing step did not sort the set of outgoing edges and remove repeated indices. This leads to large redundant communication and values being
received with much less order, but the possibly expensive sorting step could be skipped. This implementation was not expected to perform exceptionally
well, especially when large amounts of communication are required, but it was expected to perform comparably well with small communication or when the
iteration converges quickly enough for the sort of the previous implementation to not be amortized across many iterations. The results for these
timings are given below.

\begin{figure}[h]
  \begin{tabular}{| r | r | r | r | r |}
    \hline
    \multicolumn{5}{|c|}{Pagerank Total Time Results} \\
    \hline
    & & \multicolumn{1}{|c|}{\underline{Without Sorting}} & \multicolumn{1}{|c|}{\underline{With Sorting}} & \\
    Dataset & Processes & \multicolumn{1}{|r|}{Total Time (sec)} & \multicolumn{1}{|r|}{Total Time (sec)}
    & Iterations\\
    \hline
    \multirow{6}{4em}{A.graph} & Serial & 11.52 & -- & \multirow{6}{4em}{80} \\
    & 1 & 48.96 & 24.4 & \\
    & 2 & 24.64 & 12.08 & \\
    & 4 & 15.36 & 6.40 & \\
    & 8 & 7.68 & 3.92 & \\
    & 16 & 6.32 & 2.48 & \\
    & 32 & 3.28 & 1.44 & \\
    \hline
    \multirow{6}{4em}{B.graph} & Serial & 11.22 & -- & \multirow{6}{4em}{12}\\
    & 1 & 19.49 & 62.80 & \\
    & 2 & 9.18 & 31.37 & \\
    & 4 & 5.06 & 19.15 & \\
    & 8 & 3.00 & 10.09 & \\
    & 16 & 2.24 & 6.40 & \\
    & 32 & 39.36 & 20.33 & \\
    \hline
    \multirow{6}{8em}{live-journal.graph} & Serial & 39.9 & -- & \multirow{6}{4em}{100} \\
    & 1 & 91.3 & 69.4 & \\
    & 2 & 54.1 & 41.9 & \\
    & 4 & 36.0 & 28.8 & \\
    & 8 & 24.6 & 20.5 & \\
    & 16 & 18.2 & 16.1 & \\
    & 32 & 64.0 & 33.8 & \\
    \hline
  \end{tabular}
\end{figure}

As we can see, the time required for sorting in the precomputing step was amortized over enough iterations for A.graph and live-journal.graph to be
more effective than precomputing without sorting and introducing a larger communication overhead. For B.graph, very few iterations were needed for
convergence, and the sorting step took especially long, resulting in a much greater total runtime than if the sort had been skipped in the
precomputing step.

Overall, skipping the sort during precomputing was not very effective. It could have a smaller total runtime than the implementation with sorting in
certain circumstances, but these circumstances required the problem to be easier in the sense of convergence being attained in relatively few
iterations. As more difficult problems are solved, the number of iterations required will increase. The major flaw with skipping the sorting can be
seen when scaling to 32 processes in the last two datasets. When communication across a network is required, the larger message sizes causes the
algorithm to run very slowly. This fact makes the simpler implementation not scalable when running running on any reasonable computing system.

\end{document}


\end{document}
