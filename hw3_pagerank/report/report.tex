%        File: report.tex
%     Created: Tue Feb 07 05:00 PM 2017 C
% Last Change: Tue Feb 07 05:00 PM 2017 C
%

\documentclass[a4paper]{article}

\title{Computer Science 5451 Pagerank Report }
\date{3/6/17}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{changepage}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\div}{\mathrm{div}}

\newenvironment{solution}[1][]{\emph{Solution #1}}

\algnewcommand{\Or}{\textbf{ or }}
\algnewcommand{\And}{\textbf{ or }}

\begin{document}
\maketitle

\section{Parallelization}

In the Pagerank algorithm, there are two basic steps. First, each vertex must look through its outgoing edges and push its current Pagerank out along
each of these edges. Then, each vertex must combine the values pushed to it in order to obtain its new Pagerank value. In the parallel implementation,
the first step was accomplished by building an accumulator and storing the values to push in this accumulator. After filling the accumulator, the
necessary communication happens between processes, followed by the updating of Pagerank values using these communicated values.

In my implementation of parallel Pagerank, each process constructed an accumulator. The accumulator's purpose was to store the values being pushed
from each vertex in a given process in such a way to be easily communicated to other processes. One observation that speeds up the use of the accumulator is that the communication pattern is determined by the
graph and does not change from iteration to iteration. Therefore, the structures needed by the accumulator could be precomputed, reducing computation
time at each iteration. There were two different levels of precomputing that were considered for solutions to this problem. The first level determined
the number of edges that cross from the given process to all other process's in order to determine where to place pushing values in a large global
array to ensure they reached the correct process. The second level sorted the entire set of outgoing edges from a given process and eliminated
redundant edges in order to create a smaller array for storing the pushing values.

In the first precomputing implementation considered, an array was created to store the number of elements the given process needs to push to all other
processes. During each iteration, this allowed values to be stored in the accumulator by looping over all vertices to determine the value to be
pushed. For each outgoing edge from the vertex, the number of the receiving vertex is used to determine which process it belongs to, and then the
pushing value is placed in the next available entry in the portion of the accumulator being sent to the correct process. This allows the minimum
ordering on the accumulator so as to place all values being pushed to a single process in a contiguous section in memory, but within this section,
values are not in any convenient order for use by the receiving process and an individual vertex in the receiving process may receive several values
scattered throughout this array. Along with each value, another array needed to be created in order to store the vertices receiving each value so they
were used in updating the Pagerank of the correct vertex. Because vertices and edges were traversed in a deterministic manner, this array could be
precomputed, so the only array updated during each iteration was the array of pushing values. Because of the lack of order on the arrays a given
process receives, the only option for updating Pagerank values after receiving this array is to loop over the entire
array.

In the second precomputing implementation considered, all techniques given above were utilized, and additionally, the array of outgoing edges from a
given process was presorted and had all redundant receiving vertices removed. This allowed for the size of messages sent between vertices to be
reduced because when multiple vertices stored locally were pushing to the same edge, the pushed values were combined before communication rather than
making it the responsibility of the receiving process to combine them. Arrays receiving the pushed values also knew that the values it received were
ordered according to receiving vertex. This sorting allowed the accumulation of pushed values, the communication of values, and the updating of
Pagerank values from received values to all happen more quickly. However, this required an often very expensive sort to be performed.

After timing the above implementations, the method without the presorting was decided to be the most consistently effective and is the implementation
corresponding to the code turned in. The largest issue with presorting is the creation of a computation that is asymptotically large. For each
iteration, the serial time required to compute new Pagerank values is $O(|E|)$. This aligns with the $O(\frac{|E|}{p})$ computation time to update
Pagerank values in the parallel algorithm. The use of presorting introduces a $O(\frac{|E|}{p} \log \frac{|E|}{p} )$ computation. When the number of
iterations becomes large, this precompution cost can be absorbed by the increased speed of each iteration, but for small numbers of iterations, this
precomputing step dominates the computation time.

We will consider the communication cost associated to each iteration of the parallel Pagerank algorithm. Determining these costs is not a straight-forward task. This is because the communication
costs depend largely on the sparsity pattern of the graph. The communication required in the above computations is an all-to-all personalized
communication. The cost of which is
\[ (t_s + t_w m) (p - 1) \]
where $m$ is the size of the message sent from each node to every other node. There is also an all-to-all reduce in order to combine the local pieces
of the Frobenius norm of the difference in Pagerank vectors between two iterations, which is $t_s \log p + t_w m (p-1)$, but this is strictly smaller
than the term for the all-to-all personalized and will therefore be ignored. If the graph is well-behaved, each process may have roughly
$\frac{|E|}{p}$ outgoing edges, which are split evenly amongst the $p$ processes, giving $m = O \left( \frac{|E|}{p^2} \right)$. If this is the case,
when we combine with the $O \left( \frac{|E|}{p} \right)$ cost of computing the updated Pagerank values, we have a parallel runtime of
\[ T_p = O \left( \frac{|E|}{p} \right) t_c + O(p) t_s + O \left( \frac{|E|}{p} \right) t_w .\]
Multiplying by $p$, we get the processor-time product is of the same order as the serial runtime if $p^2 = O(|E|)$, leading to a cost-optimal
algorithm in this case. This analysis all depends on the assumptions made about the number of edges between different processes, which may not
necessarily hold.


\section{Results}

Following are the timing results of the Pagerank implementations. The serial implementation used is the one provided by Shaden. Values in columns
labeled ``With Sorting'' correspond to precomputing steps that presorted all outgoing edges to reduce message size. Values in columns labeled
``Without Sorting'' correspond to precomputing steps given in the first implementation above in which messages are unordered. This is the time used
for all speedup calculations.

\begin{figure}[h]
  \begin{adjustwidth}{-2.8cm}{}
  \begin{tabular}{| r | r | r r | r r | r |}
    \hline
    \multicolumn{7}{|c|}{Pagerank Timing Results} \\
    \hline
    & & \multicolumn{2}{|c|}{\underline{Without Sorting}} & \multicolumn{2}{|c|}{\underline{With Sorting}} & \\
    Dataset & Processes & \multicolumn{1}{|r}{Time/Iteration (sec)} & \multicolumn{1}{r|}{Speedup} & \multicolumn{1}{|r}{Time/Iteration (sec)} &
    \multicolumn{1}{r|}{Speedup} & Iterations\\
    \hline
    \multirow{6}{4em}{A.graph} & Serial & 0.144 & -- & -- & -- & \multirow{6}{4em}{80} \\
    & 1 & 0.612 & 0.235 & 0.305 & 0.472 & \\
    & 2 & 0.308 & 0.468 & 0.151 & 0.954 & \\
    & 4 & 0.192 & 0.750 & 0.080 & 1.80 & \\
    & 8 & 0.096 & 1.50 & 0.049 & 2.94 & \\
    & 16 & 0.079 & 1.82 &0.031 & 4.65 & \\
    & 32 & 0.041 & 3.51 & 0.018 & 8.00 & \\
    \hline
    \multirow{6}{4em}{B.graph} & Serial & 0.935 & -- & -- & -- & \multirow{6}{4em}{12}\\
    & 1 & 1.624 & 0.576 & 5.233 & 0.179 & \\
    & 2 & 0.765 & 1.22 & 2.614 & 0.358 & \\
    & 4 & 0.422 & 2.22 & 1.596 & 0.586 & \\
    & 8 & 0.250 & 3.74 & 0.841 & 1.11 & \\
    & 16 & 0.187 & 5.00 & 0.533 & 1.74 & \\
    & 32 & 2.015 & 0.464 & 1.694 & 0.552 & \\
    \hline
    \multirow{6}{8em}{live-journal.graph} & Serial & 0.399 & -- & -- & -- & \multirow{6}{4em}{100} \\
    & 1 & 0.913 & 0.437 & 0.694 & 0.575 & \\
    & 2 & 0.541 & 0.738 & 0.419 & 0.952 & \\
    & 4 & 0.360 & 1.11 & 0.288 & 1.39 & \\
    & 8 & 0.246 & 1.62 & 0.205 & 1.95 & \\
    & 16 & 0.182 & 2.19 & 0.161 & 2.48 & \\
    & 32 & 0.640 & 0.623 & 0.338 & 1.18 & \\
    \hline
  \end{tabular}
  \end{adjustwidth}
\end{figure}

As we can see, sorting was able to achieve the largest speedup seen. This was largely due to the structure of the graph in A.graph and the difficulty
of obtaining convergence. A.graph featured many edges from nearby vertices going to the same vertex. By presorting and eliminating redundant vertices
to send values to, the message size from each process was greatly reduced. This problem also took many iterations to converge, allowing the cost of
the expensive sort to be spread out over more iterations. These characteristics allowed the sorting implementation perform very well for A.graph.

For B.graph, the sorting was significantly more difficult and less effective at reducing message size. Combined with the small number of iterations to
converge, sorting was significantly outperformed by the simpler implementation without sorting. One important observation is that when scaling to 32
processes, the calculation could no longer be contained to a single machine. The communication costs between machines is much greater than the costs
of communication within a machine. Because of this, we see that the performance of the sorting implementation surpassed that of the non-sorting
implementation due to the reduced message sizes.

The data in live-journal.graph was not as structured as in A.graph, so the reduction in communication costs for the sorting implementation were not as
significant. However, the fact that the iteration cap was reached allowed the two implementation to perform fairly evenly.

While sorting in the precomputing stage did provide better performance in some cases, the dependence on the number of iterations required to converge
made it less appealing than a simpler precomputing step that doesn't sort and eliminate redundant communication. It is worth noting that the sorting
implementation did show better scaling properties when enough processes were used to spread the computation onto multiple machines due to the smaller
size of messages being sent.

\end{document}


\end{document}
