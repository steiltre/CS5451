%        File: report.tex
%     Created: Tue Feb 07 05:00 PM 2017 C
% Last Change: Tue Feb 07 05:00 PM 2017 C
%

\documentclass[a4paper]{article}

\title{Computer Science 5451 Stencil Report }
\date{}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{changepage}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\div}{\mathrm{div}}

\newenvironment{solution}[1][]{\emph{Solution #1}}

\algnewcommand{\Or}{\textbf{ or }}
\algnewcommand{\And}{\textbf{ or }}

\begin{document}
\maketitle

\section{Parallelization}

The basic idea for implementing the stencil application to run on a GPU was to allow each thread of the GPU to compute the new RGB values for a single
pixel. In this way, each thread could be performing the exact same work completely independently of all other threads. Because each pixel has the same
static stencil applied to it, the stencil matrix was stored in the constant memory to allow all threads to have easy access. The main difficulties of this
method are to effectively use the shared memory.

Because the stencil uses neighboring values to compute at a given pixel, there is a lot of overlap of data usage. For this reason, blocks of threads
first take the input values corresponding to its pixels from global memory and store them in shared memory. In order to compute the values at
the boundary of a block's tile of pixels, an extra row and column had to be appended to the block's tile of pixels in shared memory. After moving values to shared
memory, each thread could independently apply the stencil to its pixel.

The shape of a block's tile of pixels was determined by the extra pixels needed to calculate boundaries and the architecture of the GPU. If each block
computed a 1D segment of $n$ pixels, the extra row above and below the block's tile would contribute $O(n)$ extra values to be fetched from global
memory. If instead a block had a tile of $\sqrt{n} \times \sqrt{n}$ elements, the extra rows and columns would contribute only $O(\sqrt{n})$ extra
elements to fetch from global memory. For this reason, blocks were chosen to be 2-dimensional.

From above, blocks needed to be 2-dimensional to prevent extra global memory accesses, but just minimizing the number of global memory accesses by
choosing perfectly square blocks would also lead to poor performance. Threads within a block are split into 1-dimensional warps of 32 threads that perform
computations simultaneously. When a warp accesses global memory, it can coalesce these distinct memory accesses into a single operation if all threads
are accessing neighboring elements. For this reason, blocks should have a width that is a multiple of 32 in order to prevent a warp from being split
across rows of the block's tile. For this reason, blocks with $n$ threads were chosen to be rectangular with widths that were the largest multiple of 32 less than or
equal to $\sqrt{n}$. This did not have a significant impact on runtime because no enough care was taken to properly align rows of the image in memory
and fetch the halo surrounding a tile correctly.

Each block could have at most 1024
threads. Each thread would store approximately one floating point value, giving a maximum shared memory usage of roughly 4 kB per block. This is well
below the total of 48 kB, so there are no constraints placed on block sizes by the amount of shared memory used. By running with several different
block sizes, it was found that 512 threads per block appeared to give the smallest runtime.

While testing, it was seen that the time needed to send data to the GPU dominated the time to actually apply the stencil. For this reason, methods for
decreasing the time to transfer data were investigated. The biggest improvement made was using pinned memory. When using \texttt{cudaMemcpy}, data
that is not pinned is first copied to another location in RAM before being sent to the GPU. By allocating images using \texttt{cudaMallocHost}, memory
could be pinned so that it is sent directly to the GPU, skipping the intermediate copy step. As far as I understand, the issue is that the GPU cannot
copy from a location allocated first in virtual memory, so \texttt{cudaMallocHost} directly allocated physical memory. This explanation may not be
quite correct though.

Another small improvement made to transferring data to the GPU was allocating the RGB channels of the image in one contiguous location in memory. This
allowed for a single \texttt{cudaMemcpy} to be used to send data in one transfer. This did slightly increase performance, but the change was not
drastic because, while there is some overhead to starting each data transfer, there was only a total of 2 copy operations removed in each direction of
data transfer.

\section{Results}

In Figure \ref{fig:timing1}, we have the results of stencil applications using the blur operation on the CPU and GPU. Using pinned memory, we can see
that the GPU is faster than the CPU on all test images with a single stencil application. This is because the GPU can handle thousands of pixels at
once when each thread is calculating the output for a single pixel.

Looking at Figure \ref{fig:timing1}, we see that the computation time per stencil application is significantly lower on the GPU. Depending on the size
of the image, the CPU requires approximately .01 to .1 seconds for each additional stencil application while the GPU requires roughly .001 to .01
seconds for each additional stencil application. Because of this, the GPU performs significantly more FLOPs when stencils are applied many times.

\begin{figure}[h]
  \hspace{-2cm}
  \begin{tabular}{| r | r | r r | r r |}
    \hline
    \multicolumn{6}{|c|}{Stencil Application Timing Results} \\
    \hline
    Dataset & Repetitions & {CPU Time (sec)} & {CPU GFLOPs} & GPU Time (sec) & GPU GFLOPs \\
    \hline
    \multirow{6}{8em}{cavy.bmp} & 1 & 0.021 & 0.97 & 0.006 & 3.26 \\
    & 2 & 0.034 & 1.18 & 0.006 & 6.45 \\
    & 4 & 0.057 & 1.40 & 0.007 & 11.74 \\
    & 8 & 0.107 & 1.48 & 0.008 & 20.50 \\
    & 16 & 0.221 & 1.44 & 0.010 & 31.86 \\
    & 32 & 0.361 & 1.75 & 0.014 & 46.08 \\
    \hline
    \multirow{6}{8em}{dijkstra.bmp} & 1 & 0.144 & 3.27 & 0.116 & 4.06 \\
    & 2 & 0.212 & 4.45 & 0.125 & 7.56 \\
    & 4 & 0.283 & 6.67 & 0.135 & 14.04 \\
    & 8 & 0.440 & 8.58 & 0.158 & 23.88 \\
    & 16 & 0.762 & 9.92 & 0.205 & 36.87 \\
    & 32 & 1.387 & 10.89 & 0.299 & 50.48 \\
    \hline
    \multirow{6}{8em}{panorama.bmp} & 1 & 0.541 & 3.81 & 0.499 & 4.13 \\
    & 2 & 0.696 & 5.92 & 0.524 & 7.85 \\
    & 4 & 1.016 & 8.11 & 0.576 & 14.28 \\
    & 8 & 1.659 & 9.92 & 0.680 & 24.23 \\
    & 16 & 2.992 & 11.01 & 0.887 & 37.13 \\
    & 32 & 5.577 & 11.81 & 1.295 & 50.86 \\
    \hline
  \end{tabular}
  \caption{Timing results for repeated stencil application}
  \label{fig:timing1}
\end{figure}

We have included Figure \ref{fig:timing2} to highlight the performance increase from using pinned memory. In all test cases, using pinned memory
resulted in a decrease of roughly .3 to .5 seconds from the runtime. When not using pinned memory, the increase in runtime associated to each
additional stencil application is approximately the same as when using pinned memory, which is to be expected because pinned memory only helps with
the speed of transferring data to and from the GPU. This effect is significant enough that with 8 stencil applications, only panorama.bmp can have the
stencil applied faster on the GPU than the CPU. This is contrasted with pinned memory on the GPU outperforming the CPU in a single stencil application
on all test images.

\begin{figure}[h]
  \begin{tabular}{| r | r | r r |}
    \hline
    \multicolumn{4}{|c|}{Stencil Application Timing Results Without Pinned Memory} \\
    \hline
    Dataset & Repetitions & GPU Time (sec) & GPU GFLOPs \\
    \hline
    \multirow{4}{8em}{cavy.bmp} & 1 & 0.326 & 0.06 \\
    & 2 & 0.312 & 0.13 \\
    & 4 & 0.323 & 0.24 \\
    & 8 & 0.325 & 0.49 \\
    %& 16 & 0.307 & 1.03 \\
    %& 32 & 0.315 & 2.01 \\
    \hline
    \multirow{4}{8em}{dijkstra.bmp} & 1 & 0.460 & 1.03 \\
    & 2 & 0.470 & 2.01 \\
    & 4 & 0.477 & 3.96 \\
    & 8 & 0.505 & 7.48 \\
    %& 16 & 0.205 & 36.87 \\
    %& 32 & 0.299 & 50.48 \\
    \hline
    \multirow{4}{8em}{panorama.bmp} & 1 & 0.967 & 2.13 \\
    & 2 & 0.979 & 4.20 \\
    & 4 & 1.047 & 7.86 \\
    & 8 & 1.150 & 14.31 \\
    %& 16 & 0.887 & 37.13 \\
    %& 32 & 1.295 & 50.86 \\
    \hline
  \end{tabular}
  \caption{Timing results for repeated stencil application without using pinned memory}
  \label{fig:timing2}
\end{figure}

\end{document}
