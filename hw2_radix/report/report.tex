%        File: report.tex
%     Created: Tue Feb 07 05:00 PM 2017 C
% Last Change: Tue Feb 07 05:00 PM 2017 C
%

\documentclass[a4paper]{article}

\title{Computer Science 5451 Radix Sort Report }
\date{3/6/17}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{multirow}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\div}{\mathrm{div}}

\newenvironment{solution}[1][]{\emph{Solution #1}}

\algnewcommand{\Or}{\textbf{ or }}
\algnewcommand{\And}{\textbf{ or }}

\begin{document}
\maketitle

\section{Parallelization}

To parallelize radix sort, a data decomposition was used. Radix sort makes several passes over the data and sorts the numbers by a single bit or small
group of bits in each pass. This intermediate data (along with the initial data) was decomposed and split among processes using the owner computes
rule. In this case, each process was given $n/p$ numbers to place into buckets based on the value of the bits currently being sorted. After each
process sorted its individual numbers into appropriate buckets, a prefix sum was used across processes to determine where values belonged in the
``global'' array (whether or not this array was actually constructed). Due to differences in information available, the implementations in OpenMP and
MPI had some differences in synchronization.

In the OpenMP implementation, memory is shared among all threads. Therefore, information about sizes of buckets could be stored in one global array, with
each thread having a row to store its information. On the first pass through a thread's data, it computes the size of each of the local buckets it
sorts its points into and places these values in its row in the global matrix. Once all threads completed this step, a serial prefix sum step is used
to convert these bucket sizes into an array giving the locations each thread must place each of its buckets in the global array of integers. Because
this global array is in shared memory, each thread then sorts its points directly into the global array of numbers. After this, the process begins
again with each thread taking its portion of the global array to sort.

There were variations of these steps that could have been used. For instance, in the first pass each thread makes over its data, no values are sorted;
the size of each bucket is the only thing computed. Then on the next pass, values are sorted directly into the global array. Alternatively, values
could be sorted into local arrays and then copied to the global array. This was not
done because it would involve sorting the points once and then performing a copy of all of these values into the global array. In the method used,
values are sorted, but no copy operation needs to be completed. The alternative was not tested for speed and could be
faster due to some issues with caching and false sharing that were overlooked.

One issue that needs to be overcome using global arrays to store all of the data is that of false sharing. When bucket sizes are determined, each
thread is incrementing values in a shared array that has dimensions given by the number of threads and the exponential of the number of bits being sorted at once,
both of which are relatively small in this problem. Because of the small size of this array and the frequent update operations when each process is
simply incrementing counters for bucket sizes, false sharing was occuring, leading to slow data access as all data was repeatedly fetched from RAM and
not allowed to stay in cache for long. To overcome, this padding was added to each row of the shared array so each thread's data would be on a cache
line separate from data of other threads. Another solution to this issue would have been to store values in local arrays and update them to the global
array when all bucket sizes were determined.

The MPI synchronization process was much more involved due to the distributed memory and dynamic process interactions. Processes are again given a set
of points to look at. Each process determines the size of the buckets its points are placed into. Then a prefix sum is used to determine the positions
in the global array that its buckets must be placed into. In this case though, there is no global array that is constructed. So each process must send
its numbers directly to the process that is going to use them next. To accomplish this, each process must know which of the processes it needs to send
its numbers to, as well as which of the processes it will receive numbers from.

The first step to accomplishing this communication was to use an \texttt{Allgather} to give each process the global indices of buckets located on
every process. Using this information, a pair of matrices was created on each process. The first matrix for a given process contained the number of
values from each bucket that it must send to every other process. The second matrix gave the number of values the process will receive from every
bucket on every process. It was important to keep the size of communication tied to the buckets being sent from because the communication needed to be
ordered first by bucket in order to give the correct global ordering of points after communication.

To perform the communication, each bucket was iterated over, and each process performed a send on the values it needed to send from its local bucket.
These sends were ordered by the rank of processes. When a process reached its own rank, it issued all of the receives it needed to from processes
sending from the current bucket level. These receives were again ordered by process rank. This ordering of sends and receives was used to ensure
deadlock did not occur. Blocking receives were used to guarantee each process received its new values in the correct order for the next sort. The
sends were blocking as well, although this was not necessary.

\pagebreak

\section{Results}

Here are the timing results of the radix sort implementations:

\begin{center}
  \begin{tabular}{| r | r r r |}
    \hline
    \multicolumn{4}{|c|}{OpenMP Timing Results} \\
    \hline
    Dataset & Threads & Time (sec) & Speedup \\
    \hline
    \multirow{5}{4em}{1M.txt} & 1 & 0.1476 & 1 \\
    & 2 & 0.1145 & 1.29 \\
    & 4 & 0.0643 & 2.30 \\
    & 8 & 0.0368 & 4.01 \\
    & 16 & 0.0300 & 4.92 \\
    \hline
    \multirow{5}{4em}{10M.txt} & 1 & 1.5481 & 1 \\
    & 2 & 0.8409 & 1.73 \\
    & 4 & 0.4554 & 3.20 \\
    & 8 & 0.2733 & 5.34 \\
    & 16 & 0.2159 &  7.17 \\
    \hline
    \multirow{5}{4em}{100M.txt} & 1 & 22.1627 & 1 \\
    & 2 & 11.8325 & 1.87 \\
    & 4 & 5.9510 & 3.72 \\
    & 8 & 3.1818 & 6.97 \\
    & 16 & 2.0343 & 10.89 \\
    \hline
  \end{tabular}
\end{center}

\begin{center}
  \begin{tabular}{| r | r r r |}
    \hline
    \multicolumn{4}{|c|}{MPI Timing Results} \\
    \hline
    Dataset & Processes & Time (sec) & Speedup\\
    \hline
    \multirow{5}{4em}{1M.txt} & 1 & 0.1229 & 1\\
    & 2 & 0.0659 & 1.86 \\
    & 4 & 0.0360 & 3.41 \\
    & 8 & 0.0219 & 5.61 \\
    & 16 & 0.0152 & 8.09 \\
    \hline
    \multirow{5}{4em}{10M.txt} & 1 & 1.4352 & 1 \\
    & 2 & 0.7288 & 1.56 \\
    & 4 & 0.3799 & 2.99 \\
    & 8 & 0.2274 & 4.99 \\
    & 16 & 0.1420 & 7.99 \\
    \hline
    \multirow{5}{4em}{100M.txt} & 1 & 20.6660 & 1 \\
    & 2 & 11.2106 & 1.84 \\
    & 4 & 6.1703 & 3.35 \\
    & 8 & 3.6094 & 5.73 \\
    & 16 & 2.0263 & 10.20 \\
    \hline
  \end{tabular}
\end{center}

For the largest dataset, each implementation saw a speedup of over 10x. For smaller problems, reasonable speedup was seen, but was not as consistent.
The MPI implementation outperformed the OpenMP implementation on the datasets with at least ten million values. This was somewhat unexpected because
of the more cumbersome communications that needed to be performed. A small amount of this increased performance is due to the use of parallel prefix
sums.

\end{document}


\end{document}
